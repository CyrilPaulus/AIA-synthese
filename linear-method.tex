\chapter{Méthodes linéaires}

	\section{Introduction}
	Le but est de trouver un modèle qui est combinaison linéaire des entrées.

	\begin{itemize}
		\item Pour une régression, $y = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n$
		\item Pour une classification, $y = c_1$ si $w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n > 0$, $c_2$ sinon.
	\end{itemize}

	\dessin{13}

	Plusieurs méthodes existent pour trouver les coefficients $w_i$ : 

	\begin{itemize}
		\item Régression : least-square regression, ridge regression, partial least square, support vector regression, LASSO, \dots
		\item Classification : linear discriminant analysis, PLS-discriminant analysis, support vector machines, \dots \\
	\end{itemize} 

	Avantages et inconvénients :
	
	\begin{itemize}
		\item[+] simple ;
		\item[+] il existe des variantes rapides et scalable ;
		\item[+] cette méthode offre des modèles interprétatifs, à travers des poids variables (magnitude et signe) ;
		\item[-] parfois pas aussi précis que des autres méthodes (non linéaires).
	\end{itemize}

	\section{Régression linéaire}
	
	Une régression linéaire essaie d'approximer la sortie avec
	
	$$\yh(o) = w_0 + \sum_{i = 1}^n w_i a_i(o)$$
	
	Ce modèle doit être linéaire dans les paramètres, pas nécessairement dans les entrées originales. On a donc, si $\phi$ est une transformation,
	
	$$\yh(o) = w_0 + \sum_{i = 1}^n w_i \phi_i(\textbf{a}(o))$$
	
	Les entrées peuvent être
	
	\begin{itemize}
		\item des mesures,
		\item des transformations de mesures (log, racine carrée, etc),
		\item des extensions, pas exemple $a_2(p) = a^2_1(o)$, et
		\item des valeurs "dummies".
	\end{itemize}
	
	\section{Ridge regression}

	On doit trouver $w$ qui minimise (avec $\lambda > 0$) :

	$$\sum_i (y_i - wx_i)^2 + \lambda \vert \vert w \vert \vert^2$$

	Dans le cadre d'un algèbre simple, si $X$ est la matrice d'entrée et $y$ le vecteur de sortie, la solution est donnée par

	$$w^r = (X^T X + \lambda I)^{-1}X^Ty$$

	$\lambda$ permet de réguler la complexité, et d'éviter des problèmes liés à la singularité de $X^TX$.

	\section{Perceptron}

	On doit trouver $w$ qui minimise

	$$\sum_i (y_i - wx_i)^2$$

	en utilisant une descente de gradiant : étant donné un exemple d'entrainement $(x, y)$,

	$$\delta \leftarrow y - w^Tx$$
	$$\forall jw_i \leftarrow w_j + \eta \delta x_j$$

	Il s'agit d'un algorithme de type \textit{online}, c'est-à-dire qu'il traite les exemples un à un, alors qu'un algorithme de type \textit{batch} traite tous les exemples en une seule fois.

	La complexité est régulée par le taux d'apprentissage $\eta$ et le nombre d'itérations.

	\section{Extensions non linéaires}

	Plusieurs extensions existent :

	\begin{itemize}
		\item la généralisation des méthodes linéaires :
	
		$$y = w_0 + w_1 \phi_1(x_1) + w_2 \phi_2(x_2) + \dots + w_n \phi_n(x_n)$$
	
		N'importe quelle méthode linéaire peut être appliquée, mais la régulation devient plus importante.
	
		\item Réseaux de neurones artificiels, avec une seule couche cachée : si $g$ est une fonction non linéaire (par exemple une sigmoid),
	
		$$y = g(\sum_j w_J g(\sum_i w_{i, j} x_i))$$
	
		C'est une fonction non linéaire d'une combinaison linéaire de fonction non linéaires de combinaisons linéaires d'entrées.
	
		\item Méthode à base de noyaux :
	
		$$y = \sum_i w_i \phi_i(x) \Leftrightarrow y = \sum_j \alpha_j k(x_j, x)$$
	
		où $k(x, x') = \langle \phi(x), \phi(x') \rangle$, le produit scalaire dans l'espace donné et où $j$ indexe les exemples d'entraînement du modèle.
	\end{itemize}


	
	
	
	\section{Solution minimisant l'erreur quadratique moyenne}
	
	Si on pose $a_0(o) = 1 \: \forall o$, et si on dénote
	
	
	\begin{itemize}
		\item $\ab'(o_i) = (a_0(o_i), a_1(o_i), \dots , a_n(o_i))^T$, et
		\item $\wb' = (w_0, w_1, \dots , w_n)^T$,
	\end{itemize}
	
	on a le carré de l'erreur (Square Error) sur un échantillon $i$ :
	
	$$SE(o_i, \wb') = (y(o_i) - \yh(o_i))^2 = (y(o_i) - \wb'^T\ab'(o_i))^2$$
	
	On a aussi le carré de l'erreur sur tous les objets de $LS$ (Total Square Error), avec $A' = (\ab'^1, \dots , \ab'^N)$
	
	$$TSE(LS, \wb') = \sum_{i = 1}^N (y(o_i) - \wb'^T\ab'(o_i))^2 = (\yb - A'^T\wb)^T(\yb - A'^T\wb')$$
	
	$$A' = \begin{array}{c}\left. \underbrace{\begin{pmatrix}
	1 & 1 & \dots & 1 \\ 
	a_1(o_1) & a_2(o_1) & \dots & a_N(o_1) \\ 
	\vdots & \dots & \dots & \vdots \\ 
	a_1(o_n) & a_2(o_n) & \dots & a_N(o_n)
	\end{pmatrix}}_{N}\right\} n + 1\end{array} $$
	
		\subsection{SE à une dimension}
		
		Avec une seule entrée, la solution est donnée par
		
		$$(w_0*, w_1*) = arg \min_{w_0, w_1} \sum_{i = 1}^N (y(o_i) - w_0 - w_1a_1(o_i))^2$$
		
		Lorsqu'on annule la dérivée, on obtient
		
		$$w_1* = \frac{cov(a_1, y)}{\sigma^2_{a_1}}$$
		$$w_0* = \yo - w_1*\overline{a}_1$$
		
		avec $\overline{a}_1 = \frac{1}{N} \sum_{k = 1}^Na_1(o_k)$ et $\yb = \frac{1}{N} \sum_{k = 1}^N y(o_k)$. Ainsi le seul $w_i$ à trouver est $w_0$. La meilleur valeur est la moyenne des $\yh(o_i)$.
		
		$$\yh = w_0, \: w_0* = arg \min_{w_0} \underbrace{\sum_{i = 1}^N (w_0 - y(o_i))^2}_{TSE(\LS, w_0)}$$
		
		$$\frac{d}{dw_0} TSE = \sum_{i = 1}^N 2(w_0 - y(o_i)) = 0$$
		$$\Leftrightarrow 2 N w_0 = 2 \sum_{i = 1}^N y(o_i) \Leftrightarrow w_0 = \frac{1}{N} \sum_{i = 1}^N y(o_i)$$
		
		Si on substitue dans l'équation $y(o) = w_0* + w_1*a_1(o)$, on a
		
		$$\frac{y(o) - \yo}{\sigma_y} = \rho_{a_1, y} \frac{a_1(o) - \overline{a}_1}{\sigma_{a_1}}$$
		
		$\rho_{a_1, y}$ est le coefficient de corrélation entre $a_1$ et $y$, et $\sigma_y$ et $\sigma_{a_1}$ sont les déviations standards de $y$ et $a_1$.
		
		\subsection{SE à plusieurs dimensions}
		
		On cherche $\wb'$ qui minimise
		
		$$TSE(\LS, \wb') = (\yb - A'^T\wb)^T(\yb - A'^T\wb')$$
		
		Le gradient est
		
		$$\Delta_{w'} TSE(\LS, \wb') = -2 A' (\yb - A'^T\wb')$$
		
		Si on résout $\Delta_{w'} TSE(\LS, \wb'*) = 0$, on obtient
		
		$$\wb'* = (A'A'^T)^{-1}A'\yb$$
		
		A noter que $\Delta^2_{w'} TSE(\LS, \wb') = 2A'A'^T$ est semi-définie positive symétrique.
		
		[Rappel notes p3 recto]
		
		A noter qu'il existe plusieurs solutions (des vecteurs $w'* = (w_0*, w_1*)$) optimales, ce n'est pas un problème à solution unique.
		
		[slide 10/19]
		
		Considérons la matrice $(A'A'^T)$ : l'élément $i, j$ est obtenu par le produit scalaire de la $i$ème ligne et de la $j$ ème ligne de $A'$ :
		
		$$A'A'^T = N \begin{pmatrix}
		1 &  \ao_1 & \dots & \ao_n \\ 
		\ao_1 & g_{1,1} & \dots & g_{1, n} \\
		\vdots & \vdots & \ddots & \vdots \\
		\ao_n & g_{n, 1} & \dots & g_{n, n}
		\end{pmatrix} $$
		
		avec $\ao_i = \frac{1}{N} \sum_{k = 1}^N a_i(o_k)$ et $g_{i, j} = \frac{1}{N} \sum_{k = 1}^N a_i(o_k)a_j(o_k)$.
		
		Si la moyenne est nulle, $\ao_i = 0$ et les $g_{i, j}$ forment la matrice de covariance $\Sigma$
		
		\remarque{slides 12-19/19 ; ce qui suit vient de mes notes manuscrites}
		
		En conclusion, la prédiction n'est pas modifiée si on ajoute des constantes et si on effectue des combinaisons linéaires aux valeurs des attributs (tant qu'elles ne sont pas singulières).
		
		Si on retire la moyenne et l'écart-type à tous les coefficients de la matrice, on les place dans le même ordre de grandeur (abstraction des unités).
		
		
		Le problème est que plusieurs sont possible. La solution est d'utiliser $\lambda$ [slide 16/19], qui permet d'en isoler une et de mieux conditionner le problème.
		
		Augmenter $\lambda$ réduit la variance, car on tient moins compte de $A$ dans $TSE_R$. En revanche, on est moins optimal sur l'échantillon, il y a augmentation de l'erreur quadratique moyenne.
		
		\dessin{39}
		
		Augmenter $\lambda$ est bénéfique sur un échantillon de test indépendant.
		
		\remarque{Des notations sur le graphique dans mes notes manquent dans cette synthèse.}
		
		
		
		
		
		
		
% Stuff from the overview		
\begin{comment}
\subsection{Modèle linéaire}

\dessin{3}

On a par exemple

$$h(X_1, X_2) = \left\{
\begin{array}{ll}
\text{malade} & \text{si } w_0 + w_1X_1 + w_2X_2 > 0 \\
\text{sain} & \text{sinon.}
\end{array}
\right.$$

La phase d'apprentissage aura pour but de trouver les meilleurs $w_0$, $w_1$ et $w_2$ ; un modèle linéaire ne possède que trois paramètres.

\subsection{Modèle quadratique}
\dessin{4}

$$h(X_1, X_2) = \left\{
\begin{array}{ll}
\text{malade} & \text{si } w_0 + w_1X_1 + w_2X_2 + \textcolor{red}{w_3 X_1^2 + W_4X_2^2} > 0 \\
\text{sain} & \text{sinon.}
\end{array}
\right.$$

La phase d'apprentissage aura pour but de trouver les meilleurs $w_0$, $w_1$, $w_2$, $w_3$ et $w_4$.

Par facilité, on peut étendre la base de données, en ajoutant des colonnes contenant $X_1^2$ et $X_2^2$.

Un modèle quadratique est au moins aussi bon que le meilleur modèle linéaire, car on peut rendre un modèle quadratique linéaire avec $w_3 = w_4 = 0$.


\subsection{Modèle d'un réseau de neurones artificiels}
\dessin{5}
$$h(X_1, X_2) = \left\{
\begin{array}{ll}
\text{malade} & \text{si une fonction complexe de } X_1, X_2 > 0 \\
\text{sain} & \text{sinon.}
\end{array}
\right.$$
La phase d'apprentissage aura pour but de trouver les paramètres numériques de cette fonction.

\end{comment}