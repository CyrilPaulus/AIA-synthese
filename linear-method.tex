\chapter{Méthodes linéaires}

	\section{Introduction}
	Le but est de trouver un modèle qui est combinaison linéaire des entrées.

	\begin{itemize}
		\item Pour une régression, $y = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n$
		\item Pour une classification, $y = c_1$ si $w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n > 0$, $c_2$ sinon.
	\end{itemize}

	\dessin{13}

	Plusieurs méthodes existent pour trouver les coefficients $w_i$ : 

	\begin{itemize}
		\item Régression : least-square regression, ridge regression, partial least square, support vector regression, LASSO, \dots
		\item Classification : linear discriminant analysis, PLS-discriminant analysis, support vector machines, \dots \\
	\end{itemize} 

	Avantages et inconvénients :
	
	\begin{itemize}
		\item[+] simple ;
		\item[+] il existe des variantes rapides et scalable ;
		\item[+] cette méthode offre des modèles interprétatifs, à travers des poids variables (magnitude et signe) ;
		\item[-] parfois pas aussi précis que des autres méthodes (non linéaires).
	\end{itemize}

	\section{Ridge regression}
	
	Une régression linéaire essaie d'approximer la sortie avec
	
	$$\yh(o) = w_0 + \sum_{i = 1}^n w_i a_i(o)$$
	
	Ce modèle doit être linéaire dans les paramètres, pas nécessairement dans les entrées originales. On a donc, si $\phi$ est une transformation,
	
	$$\yh(o) = w_0 + \sum_{i = 1}^n w_i \phi_i(\textbf{a}(o))$$
	
	Les entrées peuvent être
	
	\begin{itemize}
		\item des mesures,
		\item des transformations de mesures (log, racine carrée, etc),
		\item des extensions, pas exemple $a_2(p) = a^2_1(o)$, et
		\item des valeurs "dummies".
	\end{itemize}
	
	
	Si on pose $a_0(o) = 1 \: \forall o$, et si on dénote
	
	\begin{itemize}
		\item $\ab'(o_i) = (a_0(o_i), a_1(o_i), \dots , a_n(o_i))^T$, et
		\item $\wb' = (w_0, w_1, \dots , w_n)^T$,
	\end{itemize}
	
	On a le carré de l'erreur (Square Error) sur un échantillon $i$ :
	
	$$SE(o_i, \wb') = (y(o_i) - \yh(o_i))^2 = (y(o_i) - \wb'^T\ab'(o_i))^2$$
	
	On a aussi le carré de l'erreur sur tous les objets de $LS$ (Total Square Error), avec $A' = (\ab'^1, \dots , \ab'^N)$
	
	$$TSE(LS, \wb') = \sum_{i = 1}^N (y(o_i) - \wb'^T\ab'(o_i))^2 = (\yb - A'^T\wb)^T(\yb - A'^T\wb')$$
	
	$$A' = \begin{array}{c}\left. \underbrace{\begin{pmatrix}
	1 & 1 & \dots & 1 \\ 
	a_1(o_1) & a_2(o_1) & \dots & a_N(o_1) \\ 
	\vdots & \dots & \dots & \vdots \\ 
	a_1(o_n) & a_2(o_n) & \dots & a_N(o_n)
	\end{pmatrix}}_{N}\right\} n + 1\end{array} $$
	
		
	On cherche $\wb'$ qui minimise
		
	$$\TSE(\LS, \wb') = (\yb - A'^T\wb)^T(\yb - A'^T\wb')$$
		
	Le gradient est
		
	$$\Delta_{w'} TSE(\LS, \wb') = -2 A' (\yb - A'^T\wb')$$
		
	Si on résout $\Delta_{w'} TSE(\LS, \wb'*) = 0$, on obtient
		
	$$\wb'* = (A'A'^T)^{-1}A'\yb$$
		
	Il existe cependant plusieurs solutions optimales, ce n'est pas un problème à solution unique. Afin d'avoir une solution unique, on va introduire un terme $\lambda > 0$ de régulation : l'erreur devient
	
	$$\TSE(\LS, \wb') = (\yb - A'^T\wb)^T(\yb - A'^T\wb') + \lambda \wb^T\wb$$
	
	Le gradient devient
	
	$$\Delta_{w} TSE(\LS, \lambda, \wb) = -2 A (\yb - A^T\wb) + 2 \lambda \wb$$
	
	La solution optimale devient alors
	
	$$\wb^*(\lambda) = (A A^T + \lambda I)^{-1} A \yb$$
	
	Cette solution est unique $\forall \lambda > 0$. Augmenter $\lambda$ réduit la variance, car on tient moins compte de $A$ dans $TSE_R$. En revanche, on est moins optimal sur l'échantillon, il y a augmentation de l'erreur quadratique moyenne.
		
	\dessin{39}
		
	Augmenter $\lambda$ est bénéfique sur un échantillon de test indépendant.
	
	En terme de temps de calcul :
	
	\begin{itemize}
		\item créer la matrice de covariance est de l'ordre de $N n^2$ opérations
		\item résoudre le système pour trouver $\wb^*$ est de l'ordre de $n^3$ opérations
		\item[$\rightarrow$] $\bigoh(n^3)$
	\end{itemize}
	
	
	C'est la méthode de régulation qui donne son nom à la \textit{ridge regression}
	
	
\section{Perceptron}
	
L'intuition derrière le perceptron est qu'un cerveau humain peut apprendre, et qu'on pourrait s'en inspirer pour développer un algorithme. Ainsi, un perceptron est la modélisation d'un neurone, qui va ensuite former des couches pour créer des réseaux de neurones.
	
\dessin{111}
	
La sortie est une somme pondérée des entrées. Le seul paramètre à adapter au problème est $\wb'$/$\wb$, qui doit minimiser
	
$$\sum_i (y_i - wx_i)^2$$

en utilisant une descente de gradient : étant donné un exemple d'entrainement $(x, y)$,

$$\delta \leftarrow y - w^Tx$$
$$\forall jw_i \leftarrow w_j + \eta \delta x_j$$

Il s'agit d'un algorithme de type \textit{online}, c'est-à-dire qu'il traite les exemples un à un, alors qu'un algorithme de type \textit{batch} traite tous les exemples en une seule fois.

La complexité est régulée par le taux d'apprentissage $\eta$ et le nombre d'itérations.

	\subsection{Algorithme du perceptron}
	
	Pour de la classification binaire, on pose que $c(o) = \pm 1$. On définit $\nu_i$ le taux d'apprentissage.
	
	\begin{itemize}
		\item on commence avec un vecteur de poids initial arbitraire, par exemple $\wb_0' = \textbf{0}$.
		\item on considère chaque éléments de $\LS$ dans une séquence cyclique ou aléatoire
		
		\item soit l'objet $o_i$ l'objet à l'étape $i$, $c(o_i)$ est sa classe et $\ab(o_i)$ son vecteur d'attributs
		\item si l'objet $o_i$ est mal classé, on ajuste le vecteur de poids avec la règle suivante :
		
		$$\wb_{i + 1}' = \wb_i' + \nu_i (c(o_i) - g_i(\ab(o_i))) \ab'(o_i)$$
	\end{itemize}
	
	% Slide 9/21 : vue géométrique, que je n'arrive pas à comprendre
	
	\subsection{Soft threshold units}
	
	La fonction d'entrée/sortie $g(\ab)$ est donnée par
	
	$$g(\ab(o)) \triangleq f(w_0 + \wb^T\ab(o)) = f(\wb'^T \ab'(o))$$
	
	$f(.)$ est une fonction d'activation qui est supposée dérivable. On a généralement pour cette fonction
	
	\begin{itemize}
		\item une sigmoïde :
		
		$$sigmoid(x) = \frac{1}{1 + \exp{-x}}$$
		
		\item une tangente hyperbolique :
		
		$$tanh(x) = \frac{\exp{x} - \exp{-x}}{\exp{x} + \exp{-x}}$$
	\end{itemize}
	
	\subsection{Descente de gradient}
	
	Slides 11 et 12/21
	
	\subsection{Propriétés}
	
	Si $\LS$ est linéairement séparable, l'algorithme convergera en un nombre fini d'étape. Sinon, il convergera avec un nombre infini d'étape si $\nu_i \rightarrow 0$.
	
	Résultats théoriques de l'algorithme de descente de gradient : 13/21


\section{Couches de perceptrons}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   _____                                          _      
%  / ____|                                        | |     
% | (___   ___  _   _ _ __ ___ ___    ___ ___   __| | ___ 
%  \___ \ / _ \| | | | '__/ __/ _ \  / __/ _ \ / _` |/ _ \
%  ____) | (_) | |_| | | | (_|  __/ | (_| (_) | (_| |  __/
% |_____/ \___/ \__,_|_|  \___\___|  \___\___/ \__,_|\___|
% | |              | |                                    
% | |__   __ _  ___| | ___   _ _ __                       
% | '_ \ / _` |/ __| |/ / | | | '_ \                      
% | |_) | (_| | (__|   <| |_| | |_) |                     
% |_.__/ \__,_|\___|_|\_\\__,_| .__/                      
%                             | |                         
%                             |_| 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
	% Ce qui suit est un résumé des slides du cours à partir du 10/19. Vu que ce n'est pas spécialement accessible ni amusant, le lecteur peut aller en parler à Denver à la fin de ce fichier.
		
	\begin{comment}
	[slide 10/19]
		
	Considérons la matrice $(A'A'^T)$ : l'élément $i, j$ est obtenu par le produit scalaire de la $i$ème ligne et de la $j$ ème ligne de $A'$ :
		
	$$A'A'^T = N \begin{pmatrix}
		1 &  \ao_1 & \dots & \ao_n \\ 
		\ao_1 & g_{1,1} & \dots & g_{1, n} \\
		\vdots & \vdots & \ddots & \vdots \\
		\ao_n & g_{n, 1} & \dots & g_{n, n}
		\end{pmatrix} $$
		
	avec $\ao_i = \frac{1}{N} \sum_{k = 1}^N a_i(o_k)$ et $g_{i, j} = \frac{1}{N} \sum_{k = 1}^N a_i(o_k)a_j(o_k)$.
		
	Si la moyenne est nulle, $\ao_i = 0$ et les $g_{i, j}$ forment la matrice de covariance $\Sigma$
		
	\remarque{slides 12-19/19 ; ce qui suit vient de mes notes manuscrites}
		
	En conclusion, la prédiction n'est pas modifiée si on ajoute des constantes et si on effectue des combinaisons linéaires aux valeurs des attributs (tant qu'elles ne sont pas singulières).
		
	Si on retire la moyenne et l'écart-type à tous les coefficients de la matrice, on les place dans le même ordre de grandeur (abstraction des unités).
				
	Le problème est que plusieurs solutions sont possible. La solution est d'utiliser $\lambda$ [slide 16/19], qui permet d'en isoler une et de mieux conditionner le problème.
		
	Augmenter $\lambda$ réduit la variance, car on tient moins compte de $A$ dans $TSE_R$. En revanche, on est moins optimal sur l'échantillon, il y a augmentation de l'erreur quadratique moyenne.
		
	\dessin{39}
		
	Augmenter $\lambda$ est bénéfique sur un échantillon de test indépendant.
		
	\end{comment}
		
		
		
	% Ce qui suit appartiennait à une première version de la synthèse, qui commencait par résumer l'overview.
	
	\begin{comment}
	\section{Extensions non linéaires}

	Plusieurs extensions existent :

	\begin{itemize}
		\item la généralisation des méthodes linéaires :
	
		$$y = w_0 + w_1 \phi_1(x_1) + w_2 \phi_2(x_2) + \dots + w_n \phi_n(x_n)$$
	
		N'importe quelle méthode linéaire peut être appliquée, mais la régulation devient plus importante.
	
		\item Réseaux de neurones artificiels, avec une seule couche cachée : si $g$ est une fonction non linéaire (par exemple une sigmoid),
	
		$$y = g(\sum_j w_J g(\sum_i w_{i, j} x_i))$$
	
		C'est une fonction non linéaire d'une combinaison linéaire de fonction non linéaires de combinaisons linéaires d'entrées.
	
		\item Méthode à base de noyaux :
	
		$$y = \sum_i w_i \phi_i(x) \Leftrightarrow y = \sum_j \alpha_j k(x_j, x)$$
	
		où $k(x, x') = \langle \phi(x), \phi(x') \rangle$, le produit scalaire dans l'espace donné et où $j$ indexe les exemples d'entraînement du modèle.
	\end{itemize}

	\end{comment}
		
% Ce qui suit vient d'une première synthèse basée sur l'overview. Ce n'est plus pertinent de l'inclure dans cette synthèse, mais le code est conservé au cas où il faudrait effectuer quelques prélèvements.

\begin{comment}
\subsection{Modèle linéaire}

\dessin{3}

On a par exemple

$$h(X_1, X_2) = \left\{
\begin{array}{ll}
\text{malade} & \text{si } w_0 + w_1X_1 + w_2X_2 > 0 \\
\text{sain} & \text{sinon.}
\end{array}
\right.$$

La phase d'apprentissage aura pour but de trouver les meilleurs $w_0$, $w_1$ et $w_2$ ; un modèle linéaire ne possède que trois paramètres.

\subsection{Modèle quadratique}
\dessin{4}

$$h(X_1, X_2) = \left\{
\begin{array}{ll}
\text{malade} & \text{si } w_0 + w_1X_1 + w_2X_2 + \textcolor{red}{w_3 X_1^2 + W_4X_2^2} > 0 \\
\text{sain} & \text{sinon.}
\end{array}
\right.$$

La phase d'apprentissage aura pour but de trouver les meilleurs $w_0$, $w_1$, $w_2$, $w_3$ et $w_4$.

Par facilité, on peut étendre la base de données, en ajoutant des colonnes contenant $X_1^2$ et $X_2^2$.

Un modèle quadratique est au moins aussi bon que le meilleur modèle linéaire, car on peut rendre un modèle quadratique linéaire avec $w_3 = w_4 = 0$.


\subsection{Modèle d'un réseau de neurones artificiels}
\dessin{5}
$$h(X_1, X_2) = \left\{
\begin{array}{ll}
\text{malade} & \text{si une fonction complexe de } X_1, X_2 > 0 \\
\text{sain} & \text{sinon.}
\end{array}
\right.$$
La phase d'apprentissage aura pour but de trouver les paramètres numériques de cette fonction.

\end{comment}


%           _                  
%          //                                                  
%         //                                                   
%      __/(                                                    
%  _.~-a  ~-.                                                  
% {_____)    `.           _..=~~~~=._                          
%       ~-_    \      _.=~           '=.                       
%          \    `._.=~            .=.   :=._                   
%           -         __         (   \   : \)                  
%            ~.      (  }       (     |   : :                  
%              `:     \ \        \    |\   ; :                 
%                \     \ }        \   / |  ;  }                
%                 `-.__//__.==~~=._\ (_/  ;  ;                 
%                     //           | |/  ;  ;                  
%                    {{       _____|_/ ;   ;        *     ___  
%                     `      ---- _=.=`   ~ _____   ||*    ____
%                             __:='    .='     ___\\||/___     
%                         ..:~____.==''                        