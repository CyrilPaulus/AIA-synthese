\chapter{Méthodes linéaires}

	\section{Introduction}
	Le but est de trouver un modèle qui est combinaison linéaire des entrées.

	\begin{itemize}
		\item Pour une régression, $y = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n$
		\item Pour une classification, $y = c_1$ si $w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n > 0$, $c_2$ sinon.
	\end{itemize}

	\dessin{13}

	Plusieurs méthodes existent pour trouver les coefficients $w_i$ : 

	\begin{itemize}
		\item Régression : least-square regression, ridge regression, partial least square, support vector regression, LASSO, \dots
		\item Classification : linear discriminant analysis, PLS-discriminant analysis, support vector machines, \dots \\
	\end{itemize} 

	\begin{itemize}
		\item[+] simple ;
		\item[+] il existe des variantes rapides et scalable ;
		\item[+] cette méthode offre des modèles interprétatifs, à travers des poids variables (magnitude et signe) ;
		\item[-] parfois pas aussi précis que des autres méthodes (non linéaires).
	\end{itemize}

		\subsection{Ridge regression}
	
		On doit trouver $w$ qui minimise (avec $\lambda > 0$) :
	
		$$\sum_i (y_i - wx_i)^2 + \lambda \vert \vert w \vert \vert^2$$
	
		Dans le cadre d'un algèbre simple, si $X$ est la matrice d'entrée et $y$ le vecteur de sortie, la solution est donnée par
	
		$$w^r = (X^T X + \lambda I)^{-1}X^Ty$$
	
		$\lambda$ permet de réguler la complexité, et d'éviter des problèmes liés à la singularité de $X^TX$.
	
		\subsection{Perceptron}
	
		On doit trouver $w$ qui minimise
	
		$$\sum_i (y_i - wx_i)^2$$
	
		en utilisant une descente de radiant : étant donné un exemple d'entrainement $(x, y)$,
	
		$$\delta \leftarrow y - w^Tx$$
		$$\forall jw_i \leftarrow w_j + \eta \delta x_j$$
	
		Il s'agit d'un algorithme de type \textit{online}, c'est-à-dire qu'il traite les exemples un à un, alors qu'un algorithme de type \textit{batch} traite tous les exemples en une seule fois.
	
		La complexité est régulée par le taux d'apprentissage $\eta$ et le nombre d'itérations.
	
		\subsection{Extensions non linéaires}
	
		Plusieurs extensions existent :
	
		\begin{itemize}
			\item la généralisation des méthodes linéaires :
		
			$$y = w_0 + w_1 \phi_1(x_1) + w_2 \phi_2(x_2) + \dots + w_n \phi_n(x_n)$$
		
			N'importe quelle méthode linéaire peut être appliquée, mais la régulation devient plus importante.
		
			\item Réseaux de neurones artificiels, avec une seule couche cachée : si $g$ est une fonction non linéaire (par exemple une sigmoid),
		
			$$y = g(\sum_j w_J g(\sum_i w_{i, j} x_i))$$
		
			C'est une fonction non linéaire d'une combinaison linéaire de fonction non linéaires de combinaisons linéaires d'entrées.
		
			\item Méthode à base de noyaux :
		
			$$y = \sum_i w_i \phi_i(x) \Leftrightarrow y = \sum_j \alpha_j k(x_j, x)$$
		
			où $k(x, x') = \langle \phi(x), \phi(x') \rangle$, le produit scalaire dans l'espace donné et où $j$ indexe les exemples d'entraînement du modèle.
		\end{itemize}

	\section{Régression linéaire}
	
	Une régression linéaire essaie d'approximer la sortie avec
	
	$$\yh(o) = w_0 + \sum_{i = 1}^n w_i a_i(o)$$
	
	Ce modèle doit être linéaire dans les paramètres, pas nécessairement dans les entrées originales. On a donc, si $\phi$ est une transformation,
	
	$$\yh(o) = w_0 + \sum_{i = 1}^n w_i \phi_i(\textbf{a}(o))$$
	
	Les entrées peuvent être
	
	\begin{itemize}
		\item des mesures,
		\item des transformations de mesures (log, racine carrée, etc),
		\item des extensions, pas exemple $a_2(p) = a^2_1(o)$, et
		\item des valeurs "dummies".
	\end{itemize}
	
	\section{Solution minimisant l'erreur quadratique moyenne}
	
	Si on pose $a_0(o) = 1 \: \forall o$, et si on dénote
	
	
	\begin{itemize}
		\item $\ab'(o_i) = (a_0(o_i), a_1(o_i), \dots , a_n(o_i))^T$, et
		\item $\wb' = (w_0, w_1, \dots , w_n)^T$,
	\end{itemize}
	
	on a le carré de l'erreur (Square Error) sur un échantillon $i$ :
	
	$$SE(o_i, \wb') = (y(o_i) - \yh(o_i))^2 = (y(o_i) - \wb'^T\ab'(o_i))^2$$
	
	On a aussi le carré de l'erreur sur tous les objets de $LS$ (Total Square Error), avec $A' = (\ab'^1, \dots , \ab'^N)$
	
	$$TSE(LS, \wb') = \sum_{i = 1}^N (y(o_i) - \wb'^T\ab'(o_i))^2 = (\yb - A'^T\wb)^T(\yb - A'^T\wb')$$
	
	$$A' = \begin{array}{c}\left. \underbrace{\begin{pmatrix}
	1 & 1 & \dots & 1 \\ 
	a_1(o_1) & a_2(o_1) & \dots & a_N(o_1) \\ 
	\vdots & \dots & \dots & \vdots \\ 
	a_1(o_n) & a_2(o_n) & \dots & a_N(o_n)
	\end{pmatrix}}_{N}\right\} n + 1\end{array} $$
	
		\subsection{SE à une dimension}
		
		Avec une seule entrée, la solution est donnée par
		
		$$(w_0*, w_1*) = arg \min_{w_0, w_1} \sum_{i = 1}^N (y(o_i) - w_0 - w_1a_1(o_i))^2$$
		
		Lorsqu'on annule la dérivée, on obtient
		
		$$w_1* = \frac{cov(a_1, y)}{\sigma^2_{a_1}}$$
		$$w_0* = \yo - w_1*\overline{a}_1$$
		
		avec $\overline{a}_1 = \frac{1}{N} \sum_{k = 1}^Na_1(o_k)$ et $\yb = \frac{1}{N} \sum_{k = 1}^N y(o_k)$. Ainsi le seul $w_i$ à trouver est $w_0$. La meilleur valeur est la moyenne des $\yh(o_i)$.
		
		$$\yh = w_0, \: w_0* = arg \min_{w_0} \underbrace{\sum_{i = 1}^N (w_0 - y(o_i))^2}_{TSE(\LS, w_0)}$$
		
		$$\frac{d}{dw_0} TSE = \sum_{i = 1}^N 2(w_0 - y(o_i)) = 0$$
		$$\Leftrightarrow 2 N w_0 = 2 \sum_{i = 1}^N y(o_i) \Leftrightarrow w_0 = \frac{1}{N} \sum_{i = 1}^N y(o_i)$$
		
		Si on substitue dans l'équation $y(o) = w_0* + w_1*a_1(o)$, on a
		
		$$\frac{y(o) - \yo}{\sigma_y} = \rho_{a_1, y} \frac{a_1(o) - \overline{a}_1}{\sigma_{a_1}}$$
		
		$\rho_{a_1, y}$ est le coefficient de corrélation entre $a_1$ et $y$, et $\sigma_y$ et $\sigma_{a_1}$ sont les déviations standards de $y$ et $a_1$.
		
		\subsection{SE à plusieurs dimensions}
