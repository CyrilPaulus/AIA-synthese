\chapter{Évaluation de modèles}

Pour un modèle donné et ayant appris sur un ensemble de données (de taille $N$), on aimerait estimer ses performances. Le but est de

\begin{itemize}
	\item sélectionner un ou plusieurs modèles (ex : déterminer la bonne complexité ou choisir entre différents algorithmes d'apprentissage).
	\item évaluer les modèles, afin d'estimer les performances sur des nouvelles données.
\end{itemize}


\section{Méthodes d'évaluation}
	\subsection{Méthode test set}

	On suppose qu'on dispose de beaucoup de données, que $N$ est grand. On divise la base de données en deux parties, une qui servira d'ensemble d'apprentissage et l'autre d'ensemble de test (par ex 70\%, 30\%). La méthode est la suivante :

	\begin{itemize}
		\item on apprend le modèle sur $\LS$
		\item on le test sur $\TS$
		\item l'estimation qui en résulte est une estimation de l'erreur d'un modèle qui aurait appris sur toute la base de données.
	\end{itemize}
	
	\dessin{65}
	
	\subsection{K-fold}
	
	La méthode du test-set n'est pas fiable sur une petite base de données car elle est basée sur un petit échantillon d'une base de données déjà réduite. De plus, on utilise l'estimation du modèle construit comme une estimation pour toute la base de données, or lorsque la base de données est très petite, apprendre le modèle sur toute la base ou sur une partie change fortement les résultats.
	
	On peut tracer la courbe d'apprentissage, où on confronte les performances à la taille de l'échantillon d'apprentissage. On voit qu'il faut que le $\LS$ soit d'une taille minimale si on veut avoir des résultats pertinents.
	
	\dessin{63}
	
	On utilise pour cela la validation k-fold : on divise aléatoirement la base de données en $k$ sous-ensembles (typiquement $k = 10$).
	
	\dessin{64}
	
	La méthode est la suivante :
	
	\begin{itemize}
		\item pour chaque sous-ensemble
		\begin{itemize}
			\item apprendre le modèle sur les objets qui ne sont pas dans le sous-ensemble
			\item calculer les prédictions du modèle sur les points du sous-ensemble
		\end{itemize}
		\item reporter l'erreur moyenne sur ces prédictions
	\end{itemize}
	
	Lorsque $k = N$, la méthode s'appelle une validation croisée \textit{leave-one-out}.
	
	Le choix de $k$ est très important :
	
	\begin{itemize}
		\item si $k = N$ :
		\begin{itemize}
			\item[+] non-biaisé : enlever un objet ne change pas trop la taille de l'échantillon d'apprentissage
			\item[-] grande variance : on dépend énormément de la base de données
			\item[-] lent : il faut entraîner $N$ modèles
		\end{itemize}
		\item si $k = 5, 10$ :
		
		\begin{itemize}
			\item[+] petite variance et rapide : on n'a que $5-10$ modèles sur peu de données
			\item[-] potentiellement biaisé (voir courbe d'apprentissage)
		\end{itemize}
	\end{itemize}
	
	\subsection{Bootstrap}
	
	Un échantillon bootstrap est un échantillon avec un remplacement ; des objets n'apparaissent pas et d'autres apparaissent plusieurs fois.
	
	\dessin{66}
	
	On a alors que
	
	$$P(o_i \in \text{ bootstrap}) = 1 - (1 - \frac{1}{N})^N \approx 1 - \frac{1}{e} = 0.632$$
	
	L'idée est d'utiliser les 30\% de données qu'il reste comme $\TS$. On peut alors estimer l'erreur de bootstrap :
	
	\begin{itemize}
		\item pour $i = 1$ jusqu'à $B$ :
		
		\begin{itemize}
			\item prendre un échantillon de boostrap $B_i$ de la base de données
			\item apprendre un modèle $f_i$ sur cet échantillon
		\end{itemize}
		
		\item pour chaque objet, calculer l'erreur de tous les modèles qui ont été construit sans lui (environ 30\%)
		\item moyenner sur tous les objets
	\end{itemize}
	
	Des améliorations existent :
	
	\begin{itemize}
		\item $.632$ bootstrap : correction pour la courbe d'apprentissage
		\item $.632+$ bootstrap : correction pour le sur-apprentissage
	\end{itemize}
	
	\subsection{Erreurs de test conditionnelles et erreurs de test attendues}
	
	Pour un modèle $\hat{f}_\LS$ donné, on a l'erreur de test conditionnelle :
	
	$$\text{Err}_\LS = \exy{L(y, \hat{f}_\LS(x))}$$
	
	On a l'erreur attendue :
	
	$$\els{\text{Err}_\LS} = \els{\exy{L(y, \hat{f}_\LS(x))}}$$
	
	Seule la méthode test set estime la première erreur, la validation croisée permet quant à elle d'estimer la seconde car on fait de l'apprentissage sur des $\LS$ différents.
	
\section{Méthodes de sélection}

Le but, pour une base de données de $N$ objets, est de déterminer le meilleur modèle possible et d'estimer l'erreur des prédictions. La méthodologie dépend encore une fois de la taille de la base.

	\subsection{Méthode test set}
	
	Si la base de données est grande, on divise aléatoirement l'ensemble d'apprentissage en trois parties : un ensemble d'apprentissage $\LS$, un ensemble de validation $\VS$ et un un ensemble de test $\TS$ (par exemple 50\%, 25\% et 25\%.
	
	\dessin{67}
	
	La méthode est la suivante :
	
	\begin{itemize}
		\item apprendre les modèles à comparer sur le $\LS$
		\item sélectionner le meilleur en basant les performances sur $\VS$
		\item le ré-entraîner sur $\LS + \VS$
		\item le tester sur $\TS$, afin d'avoir une estimation des performances
		\item le ré-entraîner sur $\LS + \VS + \TS$, afin d'avoir le modèle final
	\end{itemize}
	
	$\VS$ permet de sélectionner le modèle. Une fois que c'est fait, on réapprend sur $\LS$ et $\VS$ et on teste sur $\TS$. Le choix de la meilleure méthode (apprise sur $\LS$ et testée sur $\VS$) n'entraîne pas de sur-apprentissage, mais il y a tout de même un risque s'il y a trop de méthodes à tester (on en aurait une qui donne des résultats par coup de chance), ce qui pourrait donner une grosse erreur sur $\TS$. La solution est d'ajouter une nouvelle boucle de validation croisée ; tout choix d'échantillon peut créer un biais.
	
	\subsection{Validation croisée}
	
	On utilise deux étapes de validation croisée en k-fold.
	
	\dessin{68}
	
	CV1 est utilisé pour évaluer le modèle final, tandis que CV2 est utilisé pour la sélection du modèle.
	
	On peut également combiner la méthode test set et la validation croisée.
	
	\dessin{69}
	
	Les deux étapes sont nécessaires, car plus on compare des modèles et plus la probabilité de tomber par chance sur celui qui donne des bons résultats augmente. Les erreurs sur $\VS$ ou sur CV2 sont alors en général trop optimistes.
	
	Illustration :
	
	\dessin{70}
	
	\subsection{Méthodes analytiques}
	
	On cherche le modèle qui minimise un critère de la forme
	
	$$\text{Err}(\LS) + G(\text{complexité})$$
	
	où $G$ est une fonction monotone croissante. Le critère est dérivé de preuves théoriques.
	
	L'avantage est qu'il n'y a pas de re-entraînement, par contre on ne peut l'utiliser que pour la sélection de modèle, et on pourrait manquer le vrai optimum dans le cas d'échantillons finis.
	
\section{Biais de sélection}
	
	En général, n'importe quel choix fait en utilisant la sortie doit être dans une boucle de validation croisée.
	
	\dessin{71}
		
\section{Mesure de performance}

	\subsection{Classification binaire}
	
	Les résultats peuvent être résumés dans un tableau de contingence/matrice de confusion.
	
	\dessin{72}
	
	On définit alors
	
	$$\text{Taux d'erreur (error rate) } = \frac{FP + FN}{N + P}$$
	$$\text{Précision (accuracy) } = \frac{TP + TN}{N + P} = 1 - \text{ error rate}$$
	
	Le taux d'erreur est cependant limité : on n'a pas d'informations sur la distribution des erreurs sur les classes, et il est sensible aux changement dans la distribution des classes dans l'échantillon de test.
	
	\dessin{73}
	
	Dans le cadre de diagnostiques médicaux, on utilise des mesures plus appropriées :
	
	$$\text{Sensibilité (sensitivity/recall) } = \frac{TP}{P}$$
	$$\text{Spécificité (specificity) } = \frac{TN}{TN + FP} = 1 - \frac{FP}{N}$$
	
	La sensibilité permet de détecter le maximum de positif (on est sûr de détecter les cas positifs), tandis que la spécificité détecte le maximum de négatif. L'avantage de ces mesures est qu'elles ne dépendent pas de la proportion d'objets positifs ou négatifs.
	
	Ces mesures peuvent être portées sur une courbe ROC (Receiver operating characteristic). Si la sortie de l'algorithme d'apprentissage est un nombre (par exemple, la probabilité d'appartenir à une classe), on peut utiliser un seul afin de régler la sensibilité et la spécificité.
	
	\dessin{74}
	
	Le meilleur algorithme est le D. Si un modèle retourne toujours la classe positive, il se trouvera dans le coin supérieur droit. S'il retourne toujours la classe négative, il sera dans le coin inférieur gauche. S'il renvoie une valeur au hasard, il sera situé au centre ; la diagonale représente les choix aléatoires biaisés.
	
	Si on se trouve dans la diagonale inférieure, on peut inverser les réponses ($+ \Rightarrow -$, $- \Rightarrow +$) pour revenir dans la diagonale supérieure, et donc avoir une meilleur AUC.
	
	\dessin{75}
	
	On peut résumer une courbe ROC avec un nombre : la surface en dessous de la courbe. On peut l'interpréter comme la probabilité que deux objets choisis aléatoirement dans l'échantillon sont correctement ordonnés par le modèle, c'est-à-dire que le positif a un plus haut score que le négatif.
	
	On utilise d'autre mesures :
	
	\begin{itemize}
		\item la précision : $\frac{TP}{TP + FP}$, la proportion de bonnes prédictions parmi les prédictions positives
		\item le rappel (recall) : $\frac{TP}{TP + FN}$, la proportion de positifs qui sont détectés
		\item F-mesure = $\frac{2 * \text{précision} * \text{recall}}{\text{précision} + \text{recall}}$
	\end{itemize}
	
	\dessin{76}
	
	Le meilleur algorithme aura une courbe ROC de la forme {\pigpenfont I}, tandis que la courbe de recall aura la forme {\pigpenfont C}.
	
	\subsection{Régression}
	
		A partir du moment où on a plus d'une classe, on utilise un taux d'erreur plutôt qu'une courbe ROC.
	
		\subsubsection{Erreur quadratique}
		
		$$\frac{1}{N} \sum_{i = 1}^N (y_i - \yh_i)^2$$
		
		Le carré permet de pénaliser les très mauvaises prédictions.
		
		\subsubsection{Erreur absolue moyenne}
		
		$$\frac{1}{N} \sum_{i = 1}^N \vert y_i - \yh_i \vert$$
		
		\subsubsection{Corrélation de Pearson}
		
		$$\frac{\sum_i (y_i - \frac{1}{N} \sum_j y_j)(\yh_i - \frac{1}{N} \sum_j \yh_j)}{(N - 1)s_y s_{\yh} }$$
		 		
		\subsubsection{Corrélation de rank de Spearman}
		
		$$1 - \frac{6 \sum_i d_i^2}{N(N^2 - 1)}$$
		
		avec $d_i$ la différence de rang de $y_i$ et $\yh_i$, le rang étant l'ordre obtenu après un tri des valeurs.
		
	\subsection{Mesures de performances pour l'entraînement}
	
	Les mesures de performances pour l'entraînement peuvent être différentes des mesures de performances de test. Il y a plusieurs raisons à cela :
	
	\begin{itemize}
		\item algorithmiquement, une mesure dérivable est soumise à une optimisation de gradient (par ex le taux d'erreur et l'erreur absolue moyenne ne sont pas dérivable, l'AUC n'est pas décomposable)
		\item le sur-apprentissage : pour l'entraînement, la perte incorpore souvent un terme de pénalité pour la complexité du modèle (ce qui est inutile au moment du test). De plus, certaines mesures sont moins promptes au sur-apprentissage (par exemple la marge).
	\end{itemize}