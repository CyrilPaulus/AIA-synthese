\chapter{Machines à support vectoriel}

Cette méthode se base sur deux idées :

\begin{enumerate}
	\item la mise en place d'un classificateur à large marge, et
	\item le "noyautage" de l'espace d'entrée.
\end{enumerate}

\dessin{14}
Il faut trouver un classificateur linéaire. L'idée va être de trouver celui qui maximise la marge, c'est-à-dire la largeur du bord qui peut être étendue jusqu'à toucher une donnée.

\dessin{15}

C'est une méthode intuitivement sûre, avec une borne théorique sur l'erreur : $E(TS) < \mathcal{O}(\frac{1}{\text{margin}})$.

65 $\rightarrow$ 74

\section{Machines à support vectoriel linéaire}
	
	Soit un $\LS = \ens{(x_k, y_k)}^N_{k = 1}$, avec $y_k \in {-1, 1}$ et $x_k \in \mathbb{R}^n$. On cherche un classificateur de la forme
	
	$$\yh(x) = sgn(w^T x + b)$$
	
	qui classifie $\LS$ correctement, c'est-à-dire qui minimise
	
	$$\sum_{k = 1}^N1(y_k \neq \yh(x_k))$$
	
	\dessin{62}
	
	\subsection{Hyperplan de marge maximale}
	
	Lorsque les données sont linéairement séparables dans l'espace des features, l'hyperplan séparateur n'est pas unique.
	
	\dessin{55}
	
	Une SVM va chercher à maximiser la distance de l'hyperplan au point le plus proche dans $\LS$, autrement dit
	
	$$\underbrace{\max_{w, b} \underbrace{min \ens{\Vert x - x_k \Vert : w^T x + b = 0, k = 1, \dots , N}}_{\text{point le plus proche de la droite}}}_{\text{maximisation de la marge}}$$
	
	On maximise la marge car, intuitivement, c'est une méthode sûr. De plus, il existe des bornes théoriques sur l'erreur de généralisation qui dépend de la marge :
	
	$$Err(TS) < \bigoh (\frac{1}{\gamma}$$
	
	où $\gamma$ est la marge. Cependant, ces marges ne sont pas souvent atteintes. En pratique, une SVM fonctionne très bien.
	
	Cet algorithme conduit à un problème d'optimisation convexe, où la solution peut être écrite uniquement en terme de produits scalaires.
	
	\subsection{Problème d'optimisation}
	
	\dessin{56}
	
	$w$ est perpendiculaire à la ligne $y(x) = w^Tx + b$ :
	
	$$y(x_a) = 0 = y(x_b) \Rightarrow w^T(x_A - x_B) = 0$$
	
	Soit $x$ tel que $y(x) = 0$. La distance de l'origine à cette ligne est
	
	$$\Vert x \Vert \cos(w, x) = \Vert x \Vert \frac{w^T x}{\Vert w \Vert \: \Vert x \Vert} = \frac{w^Tx}{\Vert w \Vert} = \frac{-b}{\Vert w \Vert}$$
	
	Tout point $x$ peut être écrit comme
	
	$$x = x_{\perp} + r \frac{w}{\Vert w \Vert}$$
	
	où $x_{\perp}$ est la projection de $x$ et $\vert r \vert$ la distance entre $x$ et la ligne. En multipliant les deux membres par $w^T$, on obtient
	
	$$w^Tx = \underbrace{w^T x_{\perp}}_{\substack{-b\\ \text{car } x_{\perp} \text{ est sur} \\ \text{la droite}}} + r \frac{\overbrace{w^T w}^{\Vert w \Vert2}}{\Vert w \Vert}$$
	$$\Leftrightarrow r = \frac{w^Tx + b}{\Vert w \Vert} = \frac{y(x)}{\Vert w \Vert}$$
	
	Le problème d'optimisation peut alors être écrit comme 
	
	$$arg \max_{w, b} \ens{\frac{1}{\Vert w \Vert} \min_n [y_n . (w^T x_n + b)]}$$
	
	Si les exemples sont bien classés, $y_n$ et $w^Tx_n + b$ seront du même signe. $y_n$ permet de bien classer les valeurs, sinon on aurait une droite à l'infini (?).
	
	La solution n'est pas unique vu que l'hyperplan est inchangé si  on multiplie $w$ et $b$ par une constante $c > 0$ (par exemple, $c (w^Tx + b) = 0$ est aussi une solution). Pour imposer une unicité, on choisit typiquement $\vert w^T x + b \vert = 1$ pour le point $x$ qui est le plus proche de la surface (vecteur de support).
	
	Le problème est alors équivalent à maximiser $\frac{1}{\Vert w \Vert}$ (ou à minimiser $\Vert w \Vert$) avec les contraintes
	
	$$y_k(w^T x_k + b) \geq 1, \: \forall k = 1, \dots , N$$
	
	On a deux points qui sont les plus proches de la droite. C'est toujours le cas, sinon on pourrait encore augmenter la marge.
	
	Le problème de la SVM est équivalent à
	
	$$\min_{w, b} \varepsilon(w, b) = \frac{1}{2} \Vert w \Vert^2$$
	
	sujet aux $N$ contraintes
	
	$$y_k(w^T x_k + b) \geq 1, \: \forall k = 1, \dots , N$$

	$\Vert w \Vert$ devient $\frac{1}{2} \Vert w \Vert^2$ par facilité. Le $\frac{1}{2}$ permet de simplifier lors de la dérivation. Il s'agit d'un problème de programmation quadratique. Il existe une solution seulement si les données sont linéairement séparables, sinon des inéquations ne seront pas satisfaites.
	
	Optimisation des contraintes : 10 $\rightarrow$ 17/45
	
	\subsection{Vecteurs de support}
	
	Le problème primaire est donc
	
	$$\mathcal{L}(w, b, \alpha) = \frac{1}{2} \Vert w \Vert - \sum_{k = 1}^N \alpha_k (y_k (w^Tx_k + b) - 1)$$
	
	En accord avec les conditions complémentaires KKT (Karush-Kuhn-Tucker), le vecteur solution $w$ est tel que
	
	$$\alpha_k (y_k (w^T x_k + b) - 1) = 0, \: \forall k = 1, \dots , N$$
	
	$\alpha_k = 0$ si la contrainte est satisfaite comme une inégalité stricte $y_k (w^T x_k + b) > 1$, car c'est la façon de maximiser $\mathcal{L}$.
	
	$\alpha_k > 0$ si la contrainte est satisfaite comme une égalité $y_k(w^Tx_k + b) = 1$, auquel cas $x_k$ est le vecteur de support.
	
	Une fois que les valeurs optimales de $\alpha$ ont été déterminées, le modèle final peut être écrit comme
	
	$$\yh(x) = sgn(\sum_{i= 1}^N y_i \alpha_i x_i^T x + b)$$
	
	où les valeurs de $\alpha_k$ différentes de 0 (strictement positives) correspondent aux vecteurs de support. On a une sorte de KNN, mais on ne tient compte que des points sur la frontière, qui définissent les vecteurs de support.
	
	$b$ est calculé en exploitant le fait que pour tout $\alpha_k > 0$, on a nécessairement $y_k(wTx_k + b) - 1 = 0$.
	
	Borne pour le leave-one-out : 20/45
	
	\subsection{Sort margin}
	
	A cause du bruit ou de données isolées (outliers), les données peuvent ne pas être linéairement séparables dans l'espace des features. 
	
	\dessin{57}
	
	Les discordances sont mesurées par les variables $\xi_i \geq 0$ avec la contrainte relaxée associée $y_i(w^Tx_i + b) \geq 1 - \xi_i$. En rendant $\xi_i$ suffisamment large, la contrainte peut toujours être satisfaite :
	
	\begin{itemize}
		\item si $0 < \xi_i < 1$, la marge n'est pas satisfaite mais $x_i$ est toujours correctement classé
		\item si $\xi_i > 1$, alors $x_i$ est mal classé.
	\end{itemize}
	
	\subsubsection{Marge douce en norme 1}
	
	Le problème primal est
	
	$$\min_{w, \xi} \frac{1}{2} \Vert w \Vert^2 + C \sum_{i = 1}^N \xi_i$$
	s.t.
	
	$$y_i(w^T x_i + b) \geq 1 - \xi_i, \: \xi_i \geq 0, \: \forall i = 1, \dots , N$$
	
	où $C$ est une constante positive et qui équilibre l'objectif de maximiser la marge et de minimiser l'erreur engendrée. Ainsi, si $C = 0$, on ne pénalise pas les $\xi_i$ donc les mauvais classements. Plus $C$ est grand et plus on augmente la complexité, car les $\alpha_k$ peuvent être grands et on veut un meilleur classement (donc on prend de plus en plus en compte les $\xi_i$).
	
	\dessin{77}
	
	On a le problème dual :
	
	$$\max_\alpha \mathcal{W}(\alpha) = \sum_{k = 1}^N \alpha_k - \frac{1}{2} \sum_{i, j = 1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j$$
	
	s.t.
	
	$$0 \leq \alpha_k \leq C, \: \forall k = 1, \dots , N \text{ (contrainte de boîte)}$$
	$$\sum_{i = 1}^N \alpha_i y_i = 0$$
	
	\dessin{58}
	
	Les points dans la marge correspondent à des $\xi > 0$. La figure du milieu illustre du sur-apprentissage.
	
\section{Kernel trick}

% Transparent passé : 24/45

Les données d'apprentissage peuvent ne pas être linéairement séparables dans l'espace d'entrée. On considère alors un mapping non linéaire $\phi$ vers un nouvel espace de features.

\dessin{59}

Le problème dual devient

$$\max_\alpha \mathcal{W}(\alpha) = \sum_{k = 1}^N \alpha_k - \frac{1}{2} \sum_{i, j = 1}^N \alpha_i \alpha_j y_i y_j \phi(x_i)^T \phi(x_j)$$

Plutôt que de définir le mapping $\phi$, on peut directement définir le produit scalaire $\phi^T(x) \phi(x')$, ce qui rend le mapping implicite.

Il est possible de caractériser mathématiquement les fonctions $K(x, x')$ définies sur des paires d'objets : cette fonction est appelée un noyau (positif ou de Mercer). Il peut cependant être difficile de trouver une mesure de la similarité entre $x$ et $x'$.

Le kernel trick est que n'importe quel algorithme qui utilise les données avec uniquement des produits vectoriels peut se baser sur ce mapping implicite, en remplaçant $x^Tx'$ par $K(x, x')$. Dans le cas du SVM, on a

$$\yh(x) = sgn(\sum_{k = 1}^N y_k \alpha_k \phi(x_k)^T\phi(x) + b) = sgn(\sum_{k = 1}^N y_k \alpha_k K(x, x_k) + b)$$

où les $\alpha_k$ peuvent être déterminés en résolvant le problème de maximisation quadratique 

$$\max_\alpha \mathcal{W}(\alpha) = \sum_{k = 1}^N \alpha_k - \frac{1}{2} \sum_{i, j = 1}^N \alpha_i \alpha_j y_i y_j K(x_i, x_j)$$

sujet aux $N$ contraintes d'inégalité

$$\alpha_k \geq 0, \: \forall k = 1, \dots , N$$

et la contrainte d'égalité

$$\sum_{i = 1}^N \alpha_i y_i = 0$$

\section{Notion mathématique du noyau}

Soit $U$ un ensemble d'objets non vide. Un noyau positif est une fonction $K(., .)$

$$K(.,.) : U \times U \rightarrow \mathbb{R}$$

telle que pour tout $N \in \mathbb{N}$ et pour tout $o_1, \dots , o_N \in U$, la matrice $N \times N$

$$K : K_{i, j} = K(o_i, o_j)$$

est symétrique et définie semi-positive.

Pour tout noyau positif $K$ défini sur $U$, il existe un espace de produit scalaire $\mathcal{V}$ et une fonction $\phi(.) : U \rightarrow \mathcal{V}$ tel que

$$K(o, o') = \phi(o) \times \phi(o')$$

où l'opérateur $\times$ dénote le produit scalaire dans $\mathcal{V}$.

En général, l'espace $\mathcal{V}$ n'est pas nécessairement de dimension finie.

Le noyau défini un produit scalaire, et donc une norme et une mesure de distance sur $U$, qui est héritée de $\mathcal{V}$ :

$$d^2_U(o, o') = d^2_\mathcal{V}(\phi(o), \phi(o')) = (\phi(o) - \phi(o'))^T(\phi(o) - \phi(o'))$$
$$= \phi(o) \times \phi(o) + \phi(o') \times \phi(o') - 2 \phi(o) \times \phi(o')$$
$$= K(o, o) + K(o', o') - 2 K(o, o')$$

\section{Exemple de noyaux}

\begin{itemize}
	\item noyau constant : $K(o, o') \models 1$
	\item noyau linéaire défini sur des attributs numériques : $K(o, o') = \mathbf{a}^T(o) \mathbf{a}(o')$
	\item noyau de Hamming pour des attributs discrets : $K(o, o') = \sum_{i = 1}^m \delta_{\mathbf{a}_i(o), \mathbf{a}_i(o')}$ (nombre de composantes communes aux deux vecteurs)
	\item noyau de texte, qui calcule le nombre de sous-chaînes communes dans $o$ et $o'$ (dimension infinie si la taille du texte n'est à priori pas bornée)
	\item combinaison de noyaux :
	
	\begin{itemize}
		\item la somme de plusieurs noyaux (positifs) est toujours un noyau (positif)
		\item le produit de plusieurs noyaux est aussi un noyau
		\item noyaux polynomiaux : $\sum_{i = 0}^n a_i(K(x, x'))^i$ si $\forall i : a_i \geq 0$
	\end{itemize}
	
	\item $K(x, x') = (x^Tx')^2$ (voir 31/45)
	\item noyau gaussien : $K(x, x') = \exp{\frac{-(x - x')^T(x - x')}{2 \sigma^2}}$
\end{itemize}

\dessin{60}

\section{Méthodes à noyau}

\dessin{61}

L'approche est modulaire : on découple l'algorithme de la représentation. Beaucoup d'algorithmes peuvent utiliser des noyaux : ridge regression, PCA, k-means, etc. Des noyaux ont été défini pour plusieurs types de données : séries temporelles, images, grpahes, séquences, etc.

Les intérêts principaux des noyaux est que l'on peut travailler efficacement dans des espaces de dimension très élevée (potentiellement infinie) dès qu'un produit scalaire est facile à calculer, et qu'on peut appliquer des algorithmes classiques sur des données en toute généralité, sans être nécessairement vectorielles (par exemple construire un modèle linéaire sur un graphe).

Exemple de la ridge regression : 34 $\rightarrow$ 40 / 45

\section{Forces et faiblesses}

\begin{itemize}
	\item[+] les SVM sont motivées théoriquement
	\item[+] classifier des plus efficaces
	\item[+] implémentations efficaces pour des problèmes larges
	\item[-] modèles en boîte noire
	\item[-] le choix d'un bon noyau est difficile et critique pour atteindre une bonne précision
\end{itemize}

L'optimisation convexe est un outil très utile en apprentissage, car beaucoup de problèmes peuvent être formulés comme des problèmes d'optimisation convexe. On bénéficie également du travail important donné pour créer des algorithmes d'optimisation efficaces, et qui plus est donnent une solution unique (ce qui rend le problème plus stable).